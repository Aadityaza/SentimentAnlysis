{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Restaurant Review Sentiment Analysis usind DeepLearning\n",
        "\n",
        "This notebook performs sentiment analysis on restaurant reviews using a neural network model. It classifies reviews as positive or negative based on the sentiment expressed in the text."
      ],
      "metadata": {
        "id": "4HxDFNaFhxaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#restaurant_review_sentiment_analysis.ipynb\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p31AiV74T3PG",
        "outputId": "4d2a0028-0eff-4811-92ba-d475a2800118"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restaurant Review Sentiment Analysis\n",
        "\n",
        "# ---------------------- Import Libraries --------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- Load and Preprocess Data --------------------------------\n",
        "\n",
        "# Load the review data from .tsv file\n",
        "dataset = pd.read_csv('/content/dataset.tsv', delimiter='\\t', quoting=3)\n",
        "\n",
        "# Data cleaning and preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "    ps = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = [ps.stem(word) for word in text if word not in stop_words]\n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "\n",
        "dataset['Review'] = dataset['Review'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------- Split the Data -----------------------------------------\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(dataset['Review'], dataset['Liked'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------- Vectorization -----------------------------------------\n",
        "\n",
        "# Create a CountVectorizer object for vectorization\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the train data into a Bag-of-Words matrix\n",
        "train_matrix = vectorizer.fit_transform(train_data).toarray()\n",
        "\n",
        "# Transform the test data into a Bag-of-Words matrix\n",
        "test_matrix = vectorizer.transform(test_data).toarray()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ Build the Neural Network Model ----------------------------\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_dim=train_matrix.shape[1]))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------- Train the Model --------------------------------------\n",
        "\n",
        "# Train the model and store the training history\n",
        "history = model.fit(train_matrix, train_labels, epochs=10, batch_size=32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------- Evaluate the Model ------------------------------------\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(test_matrix, test_labels)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------- Make Predictions --------------------------------------\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_predictions = model.predict(test_matrix)\n",
        "test_predictions = (test_predictions > 0.5).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ Print Classification Report -------------------------------\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(test_labels, test_predictions)\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------ Plot Training Loss ----------------------------------\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BPGnNNvlWjE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save the model \n",
        "model.save(\"trained_model.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMSnGGppZFGO",
        "outputId": "2c9d404f-cfe2-41a0-9398-5b284fee172b",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "kNPUzVrqZRE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interface\n",
        "import gradio as gr\n",
        "def predict_sentiment(review):\n",
        "    review_matrix = vectorizer.transform([review]).toarray()\n",
        "    prediction = model.predict(review_matrix)[0][0]\n",
        "    sentiment = 'Positive' if prediction >= 0.5 else 'Negative'\n",
        "    return sentiment\n",
        "interface = gr.Interface(fn=predict_sentiment, inputs=\"text\", outputs=\"text\")\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "ZiFNzYTlZJVF",
        "outputId": "945dca83-2902-4d1b-8463-08fba3bf76ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}